---
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
    highlight: zenburn
    theme: flatly
---

```{r setup, echo=FALSE}
knitr::opts_chunk$set(tidy=TRUE, highlight=TRUE, dev="png",
               cache=TRUE, highlight=TRUE, autodep=TRUE, warning=FALSE, error=FALSE,
               message=FALSE, prompt=TRUE, comment='', fig.cap='')
```

# Overview
This is to try to estimate the power of sequencing 60 vestibular neuroma
samples using 1,222 samples of normal and breast cancer data downloaded
from TCGA. We downloaded counts generated by HTSeq for all of the samples,
and matched them up to tumor/normal status using the supplied metadata. We
were left with 1102 primary tumor samples, 113 normal samples and 7 metastatic
samples. We excluded the 7 metastatic samples from the analysis.


# Read metadata
Here we read the data in, recode the tissue type to something more sane and
then randomly select 100 normal and 100 primary tumor samples.

```{r read-metadata}
library(readr)
library(dplyr)
metadata = data.frame(fname=read_lines("htseq-files.txt"),
                      tissue=read_lines("tissue_type.txt"))
metadata$tissue = case_when(metadata$tissue == "Metastatic "~ "metastatic",
                            metadata$tissue == "Primary Tumor "~ "primary",
                            metadata$tissue == "Solid Tissue Normal "~ "normal")
metadata$tissue = as.factor(metadata$tissue)
metadata = unique(metadata)
samples = metadata %>% filter(tissue != "metastatic") %>% group_by(tissue) %>%
  sample_n(50)
samples$fname = as.character(samples$fname)
```

# Read counts

```{r read-counts}
read_counts = function(files) {
  df = read_tsv(files[1], col_names=c("gene_id", files[1]))
  for(file in files[2:length(files)]) {
    newdf = read_tsv(file, col_names=c("gene_id", file))
    df = df %>% left_join(newdf, by="gene_id")
  }
  return(df)
}
df = read_counts(file.path("data", samples$fname))

df$gene_id = 1:nrow(df)
colnames(df) = 1:ncol(df)
write_tsv(df, "scotty.tsv")
```

# Power calculation
Scoeftty is broken so we used a different method of calculating power. We
estimated the biological coficient of variation between the samples using edgeR and
then calculated power using the RNASeqPower library.

```{r cvs}
library(edgeR)
counts = as.matrix(df[,2:ncol(df)])
rownames(counts) = rownames(df)
edgeR.dgelist = DGEList(counts=counts, group=c(rep("normal", 50), rep("primary", 50)))
edgeR.dgelist = calcNormFactors(edgeR.dgelist, method = "TMM")
edgeR.dgelist = estimateCommonDisp(edgeR.dgelist)
bcv = sqrt(edgeR.dgelist$common.dispersion)
```

We'll calculate power using an estimate of 20 million reads per sample,
looking at a range of effect sizes. The effect sizes are the rows, in fold change.
The columns are at 80% power and 90% power. Below are the sample sizes needed
for 80% and 90% power and a range of small to moderate effect sizes:

```{r power}
library(RNASeqPower)
power = rnapower(depth=20, cv=bcv, effect=c(1.25, 1.50, 1.75, 2), alpha=0.05,
                 power=c(.8, .9))
knitr::kable(power)
```

This is a back of the envelope power calculation for this experiment, mostly
assuming a kind of worst case scenario where we have a lot of poorly controlled
factors that are inflating the variation between the samples. A rule of
thumb is that often BCV is < 0.4 in a carefully controlled experiment in humans.
Below is the same calculation, using 0.4 as the coefficient of variation:

```{r power-normal}
power = rnapower(depth=20, cv=0.5, effect=c(1.25, 1.50, 1.75, 2), alpha=0.05,
                 power=c(.8, .9))
knitr::kable(power)
```

The truth probably lies somewhere in between those two estimates. Either way, with
50 samples each you can expect to pick up most of the larger fold changes between
the samples if they exist.
